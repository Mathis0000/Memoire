{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "compte le nombre d'image que j'ai pour le glaucome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "root = Path(\"data/glaucome\")\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\"}\n",
    "\n",
    "# stats[dataset][label]\n",
    "stats = defaultdict(lambda: defaultdict(int))\n",
    "unknown = defaultdict(int)\n",
    "\n",
    "for p in root.rglob(\"*\"):\n",
    "    if not p.is_file() or p.suffix.lower() not in exts:\n",
    "        continue\n",
    "\n",
    "    parts = [x.lower() for x in p.parts]\n",
    "\n",
    "    # Dataset = premier dossier après data/glaucome\n",
    "    try:\n",
    "        dataset = p.relative_to(root).parts[0]\n",
    "    except Exception:\n",
    "        dataset = \"UNKNOWN\"\n",
    "\n",
    "    # Label STRICTEMENT par dossier\n",
    "    if \"glaucoma\" in parts:\n",
    "        label = \"glaucome\"\n",
    "    elif \"normal\" in parts:\n",
    "        label = \"normal\"\n",
    "    else:\n",
    "        label = \"inconnu\"\n",
    "\n",
    "    if label == \"inconnu\":\n",
    "        unknown[dataset] += 1\n",
    "    else:\n",
    "        stats[dataset][label] += 1\n",
    "\n",
    "\n",
    "# ----------- RAPPORT -----------\n",
    "print(\"RAPPORT GLAUCOME (structure-based)\\n\")\n",
    "\n",
    "total_g = total_n  = 0\n",
    "\n",
    "for dataset in sorted(set(list(stats.keys()) + list(unknown.keys()))):\n",
    "    g = stats[dataset].get(\"glaucome\", 0)\n",
    "    n = stats[dataset].get(\"normal\", 0)\n",
    "    u = unknown.get(dataset, 0)\n",
    "\n",
    "    print(f\"{dataset:<15} | glaucome: {g:4d} | normal: {n:4d} \")\n",
    "\n",
    "    total_g += g\n",
    "    total_n += n\n",
    "\n",
    "print(\"\\nTOTALS\")\n",
    "print(\"glaucome :\", total_g)\n",
    "print(\"normal   :\", total_n)\n",
    "print(\"images totales :\", total_g + total_n )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "compte le nombre d'image que j'ai pour le diabete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "csv_path = Path(\"data/diabete/trainLabels.csv/trainLabels.csv\")\n",
    "\n",
    "normal_diabete = 0\n",
    "diabete = 0\n",
    "\n",
    "with csv_path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        level = int(row[\"level\"])\n",
    "        if level == 0:\n",
    "            normal_diabete += 1\n",
    "        else:\n",
    "            diabete += 1\n",
    "\n",
    "print(\"non-diabete:\", normal_diabete)\n",
    "print(\"diabete:\", diabete)\n",
    "print(\"total:\", normal_diabete + diabete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "compte le nombre d'image mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(\n",
    "    \"data/mda/AMDNet23/\"\n",
    "    \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection/\"\n",
    "    \"AMDNet23 Dataset\"\n",
    ")\n",
    "\n",
    "train_amd = sum(1 for _ in (base / \"train\" / \"amd\").iterdir())\n",
    "valid_amd = sum(1 for _ in (base / \"valid\" / \"amd\").iterdir())\n",
    "\n",
    "\n",
    "train_normal = sum(1 for _ in (base / \"train\" / \"normal\").iterdir())\n",
    "valid_normal = sum(1 for _ in (base / \"valid\" / \"normal\").iterdir())\n",
    "\n",
    "AMDNet23_amd = train_amd + valid_amd\n",
    "AMDNet23_normal = train_normal + valid_normal\n",
    "\n",
    "print(\"AMD train :\", train_amd)\n",
    "print(\"AMD valid :\", valid_amd)\n",
    "print(\"Total AMD :\", AMDNet23_amd)\n",
    "print(\"Normal train :\", train_normal)\n",
    "print(\"Normal valid :\", valid_normal)\n",
    "print(\"Total normal :\", AMDNet23_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"data/mda/Fundus/Fundus\")\n",
    "\n",
    "fundus_armd = sum(1 for p in (base / \"ARMD\").iterdir() if p.is_file())\n",
    "fundus_normal = sum(1 for p in (base / \"NORMAL\").iterdir() if p.is_file())\n",
    "\n",
    "print(\"ARMD :\", fundus_armd)\n",
    "print(\"NORMAL :\", fundus_normal)\n",
    "print(\"TOTAL :\", fundus_armd + fundus_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"data/mda/fundus-dataset\")\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n",
    "\n",
    "def count_images(path):\n",
    "    return sum(\n",
    "        1 for p in path.iterdir()\n",
    "        if p.is_file() and p.suffix.lower() in image_exts\n",
    "    )\n",
    "\n",
    "fundus_normal = count_images(base / \"00. Normal\")\n",
    "fundus_amd = count_images(base / \"14. Age Related Macular Degeneration\")\n",
    "\n",
    "print(\"Normal :\", fundus_normal)\n",
    "print(\"AMD (14) :\", fundus_amd)\n",
    "print(\"Total :\", fundus_normal + fundus_amd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "labels_path = Path(\n",
    "    \"data/mda/hyamd-high-resolution-fundus-image-dataset\"\n",
    "    \"/hyamd-high-resolution-fundus-image-dataset-for-age-related-macular-degeneration-amd-diagnosis-1.0.0\"\n",
    "    \"/labels/labels.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "\n",
    "# 0 = non-malade, 1/2 = malade\n",
    "hyamd_normal = (df[\"AMD\"] == 0).sum()\n",
    "hyamd_amd = (df[\"AMD\"] > 0).sum()\n",
    "\n",
    "print(\"Normal :\", hyamd_normal)\n",
    "print(\"AMD :\", hyamd_amd)\n",
    "print(\"Total :\", hyamd_normal + hyamd_amd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(\"data/mda/ODIR-5K/full_df.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "ODIR_amd = (\n",
    "    df[\"labels\"]\n",
    "    .astype(str)\n",
    "    .str.contains(r\"\\bA\\b\", regex=True)\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "ODIR_normal = len(df) - ODIR_amd\n",
    "\n",
    "print(\"AMD (A):\", ODIR_amd)\n",
    "print(\"Normal:\", ODIR_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_malade = ( AMDNet23_amd + fundus_amd + hyamd_amd + ODIR_amd)\n",
    "total_normal = ( AMDNet23_normal + fundus_normal + hyamd_normal + ODIR_normal)\n",
    "print(\"Somme malade mda :\", total_malade)\n",
    "print(\"Somme normal mda :\", total_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normal:\", total_n)\n",
    "print(\"glaucoma:\", total_g)\n",
    "\n",
    "print(\"non-diabete:\", normal_diabete)\n",
    "print(\"diabete:\", diabete)\n",
    "\n",
    "print(\"Somme malade mda :\", total_malade)\n",
    "print(\"Somme normal mda :\", total_normal)\n",
    "\n",
    "print(\"\\ntotal malade: \", total_g + diabete  + total_malade )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "pour hyamd certaines data sont absentes par exemple 002456631_D_2 est dans le csv mais n'existe pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------\n",
    "# ROOTS\n",
    "# -----------------------\n",
    "root = Path(\"data\")\n",
    "mda_root = root / \"mda\"\n",
    "glaucome_root = root / \"glaucome\"\n",
    "dr_root = root / \"diabete\" / \"diabetic-retinopathy-detection\"\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\"}\n",
    "\n",
    "# -----------------------\n",
    "# UTILS\n",
    "# -----------------------\n",
    "def list_images(folder: Path):\n",
    "    folder = Path(folder)\n",
    "    if not folder.exists():\n",
    "        print(\"MANQUANT:\", folder)\n",
    "        return []\n",
    "    imgs = [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
    "    print(\"OK:\", folder, \"->\", len(imgs))\n",
    "    return imgs\n",
    "\n",
    "def uniq_existing(paths):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        rp = p.resolve()\n",
    "        if rp in seen:\n",
    "            continue\n",
    "        seen.add(rp)\n",
    "        out.append(rp)\n",
    "    return out\n",
    "\n",
    "# =========================================================\n",
    "# 1) GLAUCOME (glaucoma uniquement)\n",
    "# =========================================================\n",
    "glaucome = []\n",
    "\n",
    "# ACRIMA\n",
    "tmp = list_images(glaucome_root / \"ACRIMA\")\n",
    "glaucome += [p for p in tmp if \"glaucoma\" in [x.lower() for x in p.parts]]\n",
    "\n",
    "glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Testing\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Testing\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Testing\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"RIM-ONE\" / \"RIM-ONE\" / \"NOT PARTITIONED\" / \"glaucoma\")\n",
    "\n",
    "glaucome = uniq_existing(glaucome)\n",
    "print(\"GLAUCOME uniques :\", len(glaucome))\n",
    "\n",
    "# =========================================================\n",
    "# 2) MDA / AMD \n",
    "# =========================================================\n",
    "mda = []\n",
    "\n",
    "# ---------- AMDNet23 ----------\n",
    "mda += list_images(mda_root / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Dataset\" / \"train\" / \"amd\")\n",
    "mda += list_images(mda_root / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Dataset\" / \"valid\" / \"amd\")\n",
    "\n",
    "# ---------- Fundus ----------\n",
    "mda += list_images(mda_root / \"Fundus\" / \"Fundus\" / \"ARMD\")\n",
    "\n",
    "# ---------- fundus ----------\n",
    "mda += list_images(mda_root / \"fundus-dataset\" / \"14. Age Related Macular Degeneration\")\n",
    "\n",
    "# -------- HYAMD --------\n",
    "hyamd_root = Path(\n",
    "    \"data/mda/hyamd-high-resolution-fundus-image-dataset\"\n",
    "    \"/hyamd-high-resolution-fundus-image-dataset-for-age-related-macular-degeneration-amd-diagnosis-1.0.0\"\n",
    ")\n",
    "\n",
    "labels_hyamd = hyamd_root / \"labels\" / \"labels.csv\"\n",
    "hyamd_images = hyamd_root / \"Images\"\n",
    "\n",
    "df_hy = pd.read_csv(labels_hyamd)\n",
    "hy_amd = df_hy[df_hy[\"AMD\"] > 0][\"image_id\"].astype(str).tolist()\n",
    "\n",
    "# index des fichiers par stem normalisé\n",
    "files = [p for p in hyamd_images.iterdir() if p.is_file()]\n",
    "index = {}\n",
    "for p in files:\n",
    "    key = p.stem.rstrip(\"_\")\n",
    "    index.setdefault(key, []).append(p)\n",
    "\n",
    "hyamd_paths = []\n",
    "missing = 0\n",
    "for img_id in hy_amd:\n",
    "    if img_id in index:\n",
    "        hyamd_paths.extend(index[img_id])\n",
    "    else:\n",
    "        missing += 1\n",
    "\n",
    "# dedup\n",
    "hyamd_paths = list({p.resolve() for p in hyamd_paths})\n",
    "# Après construction de index + hy_amd\n",
    "missing_ids = [img_id for img_id in hy_amd if img_id not in index]\n",
    "print(\"Manquants:\", len(missing_ids))\n",
    "print(missing_ids[:20])\n",
    "\n",
    "print(\"HYAMD AMD attendues:\", len(hy_amd))\n",
    "print(\"HYAMD AMD ajoutées:\", len(hyamd_paths))\n",
    "print(\"HYAMD manquantes (labels sans fichier):\", missing)\n",
    "\n",
    "mda += hyamd_paths\n",
    "\n",
    "# ---------- ODIR ----------\n",
    "labels_odir = mda_root / \"ODIR-5K\"  / \"full_df.csv\"\n",
    "df_odir = pd.read_csv(labels_odir)\n",
    "\n",
    "odir_img_dir = mda_root / \"ODIR-5K\" / \"ODIR-5K\" / \"Training Images\"  # ajuste si besoin\n",
    "odir_amd_ids = df_odir[df_odir[\"labels\"].astype(str).str.contains(r\"\\bA\\b\", regex=True)][\"ID\"].tolist()\n",
    "\n",
    "odir_paths = []\n",
    "for id_ in odir_amd_ids:\n",
    "    odir_paths += [odir_img_dir / f\"{id_}_left.jpg\", odir_img_dir / f\"{id_}_right.jpg\"]\n",
    "\n",
    "odir_paths = [p for p in odir_paths if p.exists()]\n",
    "print(\"ODIR AMD:\", len(odir_paths))\n",
    "\n",
    "mda += odir_paths\n",
    "\n",
    "mda = uniq_existing(mda)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) DIABÈTE (2500)\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "labels_csv = Path(\"data/diabete/trainLabels.csv/trainLabels.csv\")\n",
    "images_dir = Path(\"data/diabete/train.zip/train\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labels_csv)\n",
    "\n",
    "\n",
    "# Construire un lookup: image(stem) -> level (int)\n",
    "label_map = dict(zip(df[\"image\"].astype(str), df[\"level\"].astype(int)))\n",
    "\n",
    "# Extensions images\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\"}\n",
    "\n",
    "# Lister les images réellement présentes (à plat)\n",
    "imgs = [p for p in images_dir.iterdir() if p.is_file() and p.suffix.lower() in exts]\n",
    "\n",
    "# Mélanger pour éviter un biais d'ordre\n",
    "random.shuffle(imgs)\n",
    "\n",
    "normal = []\n",
    "diab = []\n",
    "unknown = 0\n",
    "\n",
    "TARGET = 1500\n",
    "\n",
    "for p in imgs:\n",
    "    stem = p.stem\n",
    "    lvl = label_map.get(stem, None)\n",
    "\n",
    "    if lvl is None:\n",
    "        unknown += 1\n",
    "        continue\n",
    "\n",
    "    if lvl == 0:\n",
    "        if len(normal) < TARGET:\n",
    "            normal.append(p.resolve())\n",
    "    else:\n",
    "        if len(diab) < TARGET:\n",
    "            diab.append(p.resolve())\n",
    "\n",
    "    if len(normal) >= TARGET and len(diab) >= TARGET:\n",
    "        break\n",
    "\n",
    "print(\"Images dans le dossier :\", len(imgs))\n",
    "print(\"Sans label (non trouvées dans CSV):\", unknown)\n",
    "print(\"Normal sélectionnées :\", len(normal))\n",
    "print(\"Diabète sélectionnées :\", len(diab))\n",
    "print(\"TOTAL sélection :\", len(normal) + len(diab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaux_mda = []\n",
    "\n",
    "normaux_mda += list_images(mda_root / \"fundus-dataset\" / \"00. Normal\")\n",
    "normaux_mda += list_images(mda_root / \"Fundus\" / \"Fundus\" / \"NORMAL\")\n",
    "\n",
    "# HYAMD normaux\n",
    "hy_norm = df_hy[df_hy[\"AMD\"] == 0][\"image_id\"].astype(str).tolist()\n",
    "normaux_mda += [\n",
    "    p for img_id in hy_norm\n",
    "    for p in index.get(img_id.rstrip(\"_\"), [])\n",
    "]\n",
    "\n",
    "# ODIR normaux (non A)\n",
    "odir_norm_ids = df_odir[\n",
    "    ~df_odir[\"labels\"].astype(str).str.contains(r\"\\bA\\b\", regex=True)\n",
    "][\"ID\"].tolist()\n",
    "\n",
    "for id_ in odir_norm_ids:\n",
    "    normaux_mda += [\n",
    "        odir_img_dir / f\"{id_}_left.jpg\",\n",
    "        odir_img_dir / f\"{id_}_right.jpg\"\n",
    "    ]\n",
    "\n",
    "normaux_mda = uniq_existing(normaux_mda)\n",
    "random.shuffle(normaux_mda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaux_glaucome = []\n",
    "tmp = list_images(glaucome_root / \"ACRIMA\")\n",
    "normaux_glaucome += [p for p in tmp if \"normal\" in [x.lower() for x in p.parts]]\n",
    "normaux_glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"RIM-ONE\" / \"RIM-ONE\" / \"NOT PARTITIONED\" / \"normal\")\n",
    "normaux_glaucome = uniq_existing(normaux_glaucome)\n",
    "random.shuffle(normaux_glaucome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaux_diabete = []\n",
    "\n",
    "norm_diab_ids = df[df[\"level\"] == 0][\"image\"].astype(str).tolist()\n",
    "\n",
    "normaux_diabete = [\n",
    "    images_dir / f\"{img_id}.jpeg\"\n",
    "    for img_id in norm_diab_ids\n",
    "    if (images_dir / f\"{img_id}.jpeg\").exists()\n",
    "]\n",
    "\n",
    "normaux_diabete = uniq_existing(normaux_diabete)\n",
    "random.shuffle(normaux_diabete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL = 2500\n",
    "N_PER = N_TOTAL // 3\n",
    "\n",
    "normaux = (\n",
    "    normaux_mda[:N_PER] +\n",
    "    normaux_glaucome[:N_PER] +\n",
    "    normaux_diabete[:N_TOTAL - 2*N_PER]\n",
    ")\n",
    "\n",
    "random.shuffle(normaux)\n",
    "\n",
    "print(\"NORMAUX MDA :\", len(normaux_mda[:N_PER]))\n",
    "print(\"NORMAUX GLAUCOME :\", len(normaux_glaucome[:N_PER]))\n",
    "print(\"NORMAUX DIABÈTE :\", len(normaux_diabete[:N_TOTAL - 2*N_PER]))\n",
    "print(\"NORMAUX TOTAL :\", len(normaux))\n",
    "paths_maladies = set(\n",
    "    p.resolve()\n",
    "    for p in (glaucome + mda + diab)\n",
    ")\n",
    "\n",
    "normaux = [p for p in normaux if p.resolve() not in paths_maladies]\n",
    "\n",
    "print(\"NORMAUX après exclusion des maladies :\", len(normaux))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "il manque des normaux car duplication de path a cause du random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL = 2500\n",
    "manque = N_TOTAL - len(normaux)\n",
    "\n",
    "if manque > 0:\n",
    "    print(f\"⚠️ Il manque {manque} normaux, complétion en cours...\")\n",
    "\n",
    "    # pool global de normaux possibles\n",
    "    pool_normaux = (\n",
    "        normaux_mda +\n",
    "        normaux_glaucome +\n",
    "        normaux_diabete\n",
    "    )\n",
    "\n",
    "    # nettoyage du pool\n",
    "    pool_normaux = uniq_existing(pool_normaux)\n",
    "\n",
    "    # retirer ceux déjà utilisés ou pathologiques\n",
    "    used = set(p.resolve() for p in normaux)\n",
    "    forbidden = set(p.resolve() for p in (glaucome + mda + diab))\n",
    "\n",
    "    pool_normaux = [\n",
    "        p for p in pool_normaux\n",
    "        if p.resolve() not in used\n",
    "        and p.resolve() not in forbidden\n",
    "    ]\n",
    "\n",
    "    random.shuffle(pool_normaux)\n",
    "\n",
    "    normaux += pool_normaux[:manque]\n",
    "\n",
    "print(\"NORMAUX FINAL :\", len(normaux))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5) CSV FINAL\n",
    "# =========================================================\n",
    "rows = []\n",
    "rows += [(str(p), \"glaucome\") for p in glaucome]\n",
    "rows += [(str(p), \"mda\") for p in mda]\n",
    "rows += [(str(p), \"diabete\") for p in diab]\n",
    "rows += [(str(p), \"normaux\") for p in normaux]\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"path\", \"label\"])\n",
    "df_out[\"path\"] = df_out[\"path\"].apply(lambda p: str(Path(p).resolve()))\n",
    "df_out = df_out.drop_duplicates(subset=[\"path\"])\n",
    "\n",
    "out = root / \"dataset_selection.csv\"\n",
    "df_out.to_csv(out, index=False)\n",
    "\n",
    "print(\"\\nCSV écrit :\", out)\n",
    "print(df_out[\"label\"].value_counts())\n",
    "print(\"TOTAL :\", len(df_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "version sans image zoomer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------\n",
    "# ROOTS\n",
    "# -----------------------\n",
    "root = Path(\"data\")\n",
    "mda_root = root / \"mda\"\n",
    "glaucome_root = root / \"glaucome\"\n",
    "dr_root = root / \"diabete\" / \"diabetic-retinopathy-detection\"\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\"}\n",
    "\n",
    "# -----------------------\n",
    "# UTILS\n",
    "# -----------------------\n",
    "def list_images(folder: Path):\n",
    "    folder = Path(folder)\n",
    "    if not folder.exists():\n",
    "        print(\"MANQUANT:\", folder)\n",
    "        return []\n",
    "    imgs = [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
    "    print(\"OK:\", folder, \"->\", len(imgs))\n",
    "    return imgs\n",
    "\n",
    "def uniq_existing(paths):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        rp = p.resolve()\n",
    "        if rp in seen:\n",
    "            continue\n",
    "        seen.add(rp)\n",
    "        out.append(rp)\n",
    "    return out\n",
    "\n",
    "# =========================================================\n",
    "# 1) GLAUCOME (glaucoma uniquement)\n",
    "# =========================================================\n",
    "glaucome = []\n",
    "\n",
    "glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Testing\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Testing\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Training\" / \"glaucoma\")\n",
    "glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Testing\" / \"glaucoma\")\n",
    "\n",
    "glaucome = uniq_existing(glaucome)\n",
    "print(\"GLAUCOME uniques :\", len(glaucome))\n",
    "\n",
    "# =========================================================\n",
    "# 2) MDA / AMD \n",
    "# =========================================================\n",
    "mda = []\n",
    "\n",
    "# ---------- AMDNet23 ----------\n",
    "mda += list_images(mda_root / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Dataset\" / \"train\" / \"amd\")\n",
    "mda += list_images(mda_root / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\" / \"AMDNet23 Dataset\" / \"valid\" / \"amd\")\n",
    "\n",
    "# ---------- Fundus ----------\n",
    "mda += list_images(mda_root / \"Fundus\" / \"Fundus\" / \"ARMD\")\n",
    "\n",
    "# ---------- fundus ----------\n",
    "mda += list_images(mda_root / \"fundus-dataset\" / \"14. Age Related Macular Degeneration\")\n",
    "\n",
    "# -------- HYAMD --------\n",
    "hyamd_root = Path(\n",
    "    \"data/mda/hyamd-high-resolution-fundus-image-dataset\"\n",
    "    \"/hyamd-high-resolution-fundus-image-dataset-for-age-related-macular-degeneration-amd-diagnosis-1.0.0\"\n",
    ")\n",
    "\n",
    "labels_hyamd = hyamd_root / \"labels\" / \"labels.csv\"\n",
    "hyamd_images = hyamd_root / \"Images\"\n",
    "\n",
    "df_hy = pd.read_csv(labels_hyamd)\n",
    "hy_amd = df_hy[df_hy[\"AMD\"] > 0][\"image_id\"].astype(str).tolist()\n",
    "\n",
    "# index des fichiers par stem normalisé\n",
    "files = [p for p in hyamd_images.iterdir() if p.is_file()]\n",
    "index = {}\n",
    "for p in files:\n",
    "    key = p.stem.rstrip(\"_\")\n",
    "    index.setdefault(key, []).append(p)\n",
    "\n",
    "hyamd_paths = []\n",
    "missing = 0\n",
    "for img_id in hy_amd:\n",
    "    if img_id in index:\n",
    "        hyamd_paths.extend(index[img_id])\n",
    "    else:\n",
    "        missing += 1\n",
    "\n",
    "# dedup\n",
    "hyamd_paths = list({p.resolve() for p in hyamd_paths})\n",
    "# Après construction de index + hy_amd\n",
    "missing_ids = [img_id for img_id in hy_amd if img_id not in index]\n",
    "print(\"Manquants:\", len(missing_ids))\n",
    "print(missing_ids[:20])\n",
    "\n",
    "print(\"HYAMD AMD attendues:\", len(hy_amd))\n",
    "print(\"HYAMD AMD ajoutées:\", len(hyamd_paths))\n",
    "print(\"HYAMD manquantes (labels sans fichier):\", missing)\n",
    "\n",
    "mda += hyamd_paths\n",
    "\n",
    "# ---------- ODIR ----------\n",
    "labels_odir = mda_root / \"ODIR-5K\"  / \"full_df.csv\"\n",
    "df_odir = pd.read_csv(labels_odir)\n",
    "\n",
    "odir_img_dir = mda_root / \"ODIR-5K\" / \"ODIR-5K\" / \"Training Images\"  # ajuste si besoin\n",
    "odir_amd_ids = df_odir[df_odir[\"labels\"].astype(str).str.contains(r\"\\bA\\b\", regex=True)][\"ID\"].tolist()\n",
    "\n",
    "odir_paths = []\n",
    "for id_ in odir_amd_ids:\n",
    "    odir_paths += [odir_img_dir / f\"{id_}_left.jpg\", odir_img_dir / f\"{id_}_right.jpg\"]\n",
    "\n",
    "odir_paths = [p for p in odir_paths if p.exists()]\n",
    "print(\"ODIR AMD:\", len(odir_paths))\n",
    "\n",
    "mda += odir_paths\n",
    "\n",
    "mda = uniq_existing(mda)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) DIABÈTE (2500)\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "labels_csv = Path(\"data/diabete/trainLabels.csv/trainLabels.csv\")\n",
    "images_dir = Path(\"data/diabete/train.zip/train\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labels_csv)\n",
    "\n",
    "# Construire un lookup: image(stem) -> level (int)\n",
    "label_map = dict(zip(df[\"image\"].astype(str), df[\"level\"].astype(int)))\n",
    "\n",
    "# Extensions images\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\"}\n",
    "\n",
    "# Lister les images réellement présentes (à plat)\n",
    "imgs = [p for p in images_dir.iterdir() if p.is_file() and p.suffix.lower() in exts]\n",
    "\n",
    "# Mélanger pour éviter un biais d'ordre\n",
    "random.shuffle(imgs)\n",
    "\n",
    "normal = []\n",
    "diab = []\n",
    "unknown = 0\n",
    "\n",
    "TARGET = 1500\n",
    "\n",
    "for p in imgs:\n",
    "    stem = p.stem\n",
    "    lvl = label_map.get(stem, None)\n",
    "\n",
    "    if lvl is None:\n",
    "        unknown += 1\n",
    "        continue\n",
    "\n",
    "    if lvl == 0:\n",
    "        if len(normal) < TARGET:\n",
    "            normal.append(p.resolve())\n",
    "    else:\n",
    "        if len(diab) < TARGET:\n",
    "            diab.append(p.resolve())\n",
    "\n",
    "    if len(normal) >= TARGET and len(diab) >= TARGET:\n",
    "        break\n",
    "\n",
    "print(\"Images dans le dossier :\", len(imgs))\n",
    "print(\"Sans label (non trouvées dans CSV):\", unknown)\n",
    "print(\"Normal sélectionnées :\", len(normal))\n",
    "print(\"Diabète sélectionnées :\", len(diab))\n",
    "print(\"TOTAL sélection :\", len(normal) + len(diab))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaux_glaucome = []\n",
    "normaux_glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"DRISHTI-GS\" / \"DRISHTI-GS\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"LAG\" / \"LAG\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Training\" / \"normal\")\n",
    "normaux_glaucome += list_images(glaucome_root / \"ORIGA\" / \"ORIGA\" / \"Testing\" / \"normal\")\n",
    "normaux_glaucome = uniq_existing(normaux_glaucome)\n",
    "random.shuffle(normaux_glaucome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOTAL = 1500\n",
    "N_PER = N_TOTAL // 3\n",
    "\n",
    "normaux = (\n",
    "    normaux_mda[:N_PER] +\n",
    "    normaux_glaucome[:N_PER] +\n",
    "    normaux_diabete[:N_TOTAL - 2*N_PER]\n",
    ")\n",
    "\n",
    "random.shuffle(normaux)\n",
    "\n",
    "print(\"NORMAUX MDA :\", len(normaux_mda[:N_PER]))\n",
    "print(\"NORMAUX GLAUCOME :\", len(normaux_glaucome[:N_PER]))\n",
    "print(\"NORMAUX DIABÈTE :\", len(normaux_diabete[:N_TOTAL - 2*N_PER]))\n",
    "print(\"NORMAUX TOTAL :\", len(normaux))\n",
    "paths_maladies = set(\n",
    "    p.resolve()\n",
    "    for p in (glaucome + mda + diab)\n",
    ")\n",
    "\n",
    "normaux = [p for p in normaux if p.resolve() not in paths_maladies]\n",
    "\n",
    "print(\"NORMAUX après exclusion des maladies :\", len(normaux))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5) CSV FINAL\n",
    "# ========================================================\n",
    "random.shuffle(glaucome)\n",
    "glaucome = glaucome[:1500]\n",
    "rows = []\n",
    "rows += [(str(p), \"glaucome\") for p in glaucome]\n",
    "rows += [(str(p), \"mda\") for p in mda]\n",
    "rows += [(str(p), \"diabete\") for p in diab]\n",
    "rows += [(str(p), \"normaux\") for p in normaux]\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"path\", \"label\"])\n",
    "df_out[\"path\"] = df_out[\"path\"].apply(lambda p: str(Path(p).resolve()))\n",
    "df_out = df_out.drop_duplicates(subset=[\"path\"])\n",
    "\n",
    "out = root / \"dataset_selection_non_zoomer.csv\"\n",
    "df_out.to_csv(out, index=False)\n",
    "\n",
    "print(\"\\nCSV écrit :\", out)\n",
    "print(df_out[\"label\"].value_counts())\n",
    "print(\"TOTAL :\", len(df_out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
