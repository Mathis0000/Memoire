{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv(\"../data/dataset_selection_non_zoomer.csv\")\n",
    "\n",
    "# Aperçu du dataset\n",
    "print(\"Aperçu du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nInformations générales :\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre total d'images : {len(df)}\")\n",
    "print(f\"Nombre de colonnes : {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'images par label\n",
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "print(\"\\nRépartition des labels :\")\n",
    "print(class_counts)\n",
    "\n",
    "# Pourcentage par label\n",
    "class_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPourcentage par label :\")\n",
    "print(class_percent.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title(\"Distribution des labels\")\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classe in df['label'].unique():\n",
    "    print(f\"\\nExemples pour la classe '{classe}' :\")\n",
    "    print(df[df['label'] == classe]['path'].head(3).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_test, df_val = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    stratify=df_test['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tailles des splits :\")\n",
    "print(f\"Train : {len(df_train)}\")\n",
    "print(f\"Test : {len(df_test)}\")\n",
    "print(f\"Validation : {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_distribution(name, data):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(data['label'].value_counts(normalize=True).round(3))\n",
    "\n",
    "show_distribution(\"Train\", df_train)\n",
    "show_distribution(\"Test\", df_test)\n",
    "show_distribution(\"Validation\", df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "print(sys.executable)\n",
    "print(torch.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.randn(1000, 1000, device=\"cuda\")\n",
    "print(\"OK:\", x.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PyTorch pipeline COMPLET (ResNet34 pré-entraîné + class weights + AMP + AdamW + ReduceLROnPlateau + EarlyStopping)\n",
    "# =========================\n",
    "\n",
    "import os, numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "IMG = 224\n",
    "BATCH = 32\n",
    "EPOCHS = 300\n",
    "PATIENCE = 20\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 0) Device\n",
    "# -------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Transforms (ImageNet)\n",
    "# -------------------------------------------------\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG+32),\n",
    "    transforms.CenterCrop(IMG),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Dataset depuis DataFrame splits: df_train/df_val/df_test\n",
    "#    df_* doit contenir colonnes: \"path\" et \"label\"\n",
    "# -------------------------------------------------\n",
    "class FromSamples(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = np.array(paths)\n",
    "        self.labels = np.array(labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, int(self.labels[i])\n",
    "\n",
    "# --- Vérif colonnes ---\n",
    "for name, d in [(\"df_train\", df_train), (\"df_val\", df_val), (\"df_test\", df_test)]:\n",
    "    assert \"path\" in d.columns and \"label\" in d.columns, f\"{name} doit avoir colonnes 'path' et 'label'\"\n",
    "\n",
    "# --- Remap labels -> {0..C-1} (IMPORTANT pour CrossEntropyLoss) ---\n",
    "orig_classes = sorted(df_train[\"label\"].unique().tolist())\n",
    "label_map = {c: i for i, c in enumerate(orig_classes)}\n",
    "print(\"Label mapping:\", label_map)\n",
    "\n",
    "df_train = df_train.copy()\n",
    "df_val   = df_val.copy()\n",
    "df_test  = df_test.copy()\n",
    "\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label_map)\n",
    "df_val[\"label\"]   = df_val[\"label\"].map(label_map)\n",
    "df_test[\"label\"]  = df_test[\"label\"].map(label_map)\n",
    "\n",
    "# --- Datasets ---\n",
    "ds_train = FromSamples(df_train[\"path\"].values, df_train[\"label\"].values, train_tfms)\n",
    "ds_val   = FromSamples(df_val[\"path\"].values,   df_val[\"label\"].values,   eval_tfms)\n",
    "ds_test  = FromSamples(df_test[\"path\"].values,  df_test[\"label\"].values,  eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(ds_val,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(ds_test,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Class weights (robuste, sans division par zéro)\n",
    "# -------------------------------------------------\n",
    "cnt = Counter(df_train[\"label\"].values.tolist())\n",
    "num_classes = len(orig_classes)\n",
    "\n",
    "print(\"Train counts:\", cnt)\n",
    "if len(cnt) != num_classes:\n",
    "    print(\"⚠️ Attention: une ou plusieurs classes sont absentes du train. \"\n",
    "          \"Impossible de calculer des poids pour une classe absente. \"\n",
    "          \"On calcule quand même des poids, mais ton split est à revoir.\")\n",
    "\n",
    "# On construit un vecteur de taille num_classes, avec fallback si cnt[c]==0\n",
    "weights_list = []\n",
    "for c in range(num_classes):\n",
    "    if cnt.get(c, 0) == 0:\n",
    "        # fallback: poids = 1.0 (tu peux aussi mettre très grand, mais ça casse souvent l'entraînement)\n",
    "        weights_list.append(1.0)\n",
    "    else:\n",
    "        weights_list.append(len(df_train) / (num_classes * cnt[c]))\n",
    "\n",
    "weights_ce = torch.tensor(weights_list, dtype=torch.float32, device=device)\n",
    "print(\"Class weights:\", weights_ce.tolist())\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Modèle: ResNet34 pré-entraîné ImageNet + tête 2 classes (ou num_classes)\n",
    "# -------------------------------------------------\n",
    "weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "model = models.resnet34(weights=weights)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_ce)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10, verbose=True)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Eval / Train loops + Early stopping\n",
    "# -------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    loss_sum, correct, n = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct  += (logits.argmax(1) == y).sum().item()\n",
    "        n += x.size(0)\n",
    "    return loss_sum / max(n, 1), correct / max(n, 1)\n",
    "\n",
    "best_acc, best_state = 0.0, None\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    run_loss, n = 0.0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        run_loss += loss.item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "\n",
    "    train_loss = run_loss / max(n, 1)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_acc {val_acc:.4f} | lr {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        wait = 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "all_y, all_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_y.append(y.numpy())\n",
    "\n",
    "y_true = np.concatenate(all_y, axis=0)\n",
    "y_pred = np.concatenate(all_pred, axis=0)\n",
    "\n",
    "classes = orig_classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) METRICS\n",
    "# ----------------------------\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "prec_weighted = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "rec_weighted  = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1_weighted   = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"\\n=== Scores globaux ===\")\n",
    "print(f\"Accuracy            : {acc:.4f}\")\n",
    "print(f\"Balanced accuracy   : {bacc:.4f}\")\n",
    "print(f\"Precision (macro)   : {prec_macro:.4f}\")\n",
    "print(f\"Recall (macro)      : {rec_macro:.4f}\")\n",
    "print(f\"F1 (macro)          : {f1_macro:.4f}\")\n",
    "print(f\"Precision (weighted): {prec_weighted:.4f}\")\n",
    "print(f\"Recall (weighted)   : {rec_weighted:.4f}\")\n",
    "print(f\"F1 (weighted)       : {f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n=== Rapport par classe ===\")\n",
    "target_names = [str(c) for c in classes]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "# ----------------------------\n",
    "# 8) CONFUSION MATRIX (labels réels, ordre forcé)\n",
    "# ----------------------------\n",
    "label_indices = list(range(num_classes))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=label_indices)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp.plot(ax=ax, values_format=\"d\", cmap=None, colorbar=False)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel(\"Label prédit\")\n",
    "plt.ylabel(\"Vrai label\")\n",
    "plt.title(\"Matrice de confusion (labels réels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#montre des exemples d'images mal classées\n",
    "import numpy as np\n",
    "misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "print(f\"Nombre d'images mal classées : {len(misclassified_indices)}\")   \n",
    "for idx in misclassified_indices[:5]:  # Affiche les 5 premières erreurs\n",
    "    img_path = df_test.iloc[idx]['path']\n",
    "    true_label = classes[y_true[idx]]\n",
    "    pred_label = classes[y_pred[idx]]\n",
    "\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG, IMG])\n",
    "\n",
    "    plt.imshow(img.numpy().astype(\"uint8\"))\n",
    "    plt.title(f\"Vrai: {true_label} | Prédit: {pred_label} | Path: {img_path}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
