{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv(\"../data/dataset_selection.csv\")\n",
    "\n",
    "# Aper√ßu du dataset\n",
    "print(\"Aper√ßu du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nInformations g√©n√©rales :\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre total d'images : {len(df)}\")\n",
    "print(f\"Nombre de colonnes : {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'images par label\n",
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "print(\"\\nR√©partition des labels :\")\n",
    "print(class_counts)\n",
    "\n",
    "# Pourcentage par label\n",
    "class_percent = df['label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPourcentage par label :\")\n",
    "print(class_percent.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title(\"Distribution des labels\")\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classe in df['label'].unique():\n",
    "    print(f\"\\nExemples pour la classe '{classe}' :\")\n",
    "    print(df[df['label'] == classe]['path'].head(3).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tailles des splits :\")\n",
    "print(f\"Train : {len(df_train)}\")\n",
    "print(f\"Test : {len(df_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_distribution(name, data):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(data['label'].value_counts(normalize=True).round(3))\n",
    "\n",
    "show_distribution(\"Train\", df_train)\n",
    "show_distribution(\"Test\", df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def augment_image(img):\n",
    "    aug_images = []\n",
    "\n",
    "    # rotation\n",
    "    for angle in [-15, 15]:\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1)\n",
    "        aug_images.append(cv2.warpAffine(img, M, (w, h)))\n",
    "\n",
    "    # flip\n",
    "    aug_images.append(cv2.flip(img, 1))\n",
    "\n",
    "    return aug_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image_gray(path):\n",
    "    img = Image.open(path).convert(\"L\")\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Version avec extraction de caracteristique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog(image):\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    features = hog(\n",
    "        image,\n",
    "        orientations=9,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2),\n",
    "        block_norm='L2-Hys'\n",
    "    )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    img = load_image_gray(row['path'])\n",
    "\n",
    "    # image originale\n",
    "    X_train.append(extract_hog(img))\n",
    "    y_train.append(row['label'])\n",
    "\n",
    "    # data augmentation\n",
    "    for aug in augment_image(img):\n",
    "        X_train.append(extract_hog(aug))\n",
    "        y_train.append(row['label'])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(\"Train :\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = [], []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    img = load_image_gray(row['path'])\n",
    "    X_test.append(extract_hog(img))\n",
    "    y_test.append(row['label'])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Test :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lr = LogisticRegression(\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"tol\": [1e-4, 1e-3, 1e-2],\n",
    "        \"max_iter\": [2000, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs param√®tres :\", grid.best_params_)\n",
    "print(\"Meilleur score CV (F1-macro) :\", grid.best_score_)\n",
    "best_lr = grid.best_estimator_\n",
    "y_pred = best_lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n===== Logistic Regression (Best Model) =====\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred, labels=best_lr.classes_)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=best_lr.classes_,\n",
    "    yticklabels=best_lr.classes_\n",
    ")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de confusion - Logistic Regression (Best)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "prec_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "rec_macro  = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_macro   = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "prec_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "rec_weighted  = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1_weighted   = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision macro: {prec_macro:.4f} | weighted: {prec_weighted:.4f}\")\n",
    "print(f\"Recall macro:    {rec_macro:.4f} | weighted: {rec_weighted:.4f}\")\n",
    "print(f\"F1 macro:        {f1_macro:.4f} | weighted: {f1_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificities = []\n",
    "\n",
    "for i in range(len(best_lr.classes_)):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    specificities.append(spec)\n",
    "\n",
    "spec_macro = np.mean(specificities)\n",
    "\n",
    "print(f\"Specificity macro: {spec_macro:.4f}\")\n",
    "print(\"Specificity par classe:\")\n",
    "print(dict(zip(best_lr.classes_, specificities)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=best_lr.classes_,\n",
    "    yticklabels=best_lr.classes_\n",
    ")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de confusion normalis√©e - Logistic Regression (Best)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_lr, \"logistic_regression_best.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"Meilleur mod√®le et scaler sauvegard√©s üíæ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Seulement PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(image, size=(128, 128)):\n",
    "    image = cv2.resize(image, size)\n",
    "    return image.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    img = load_image_gray(row[\"path\"])\n",
    "\n",
    "    # image originale\n",
    "    X_train.append(image_to_vector(img))\n",
    "    y_train.append(row[\"label\"])\n",
    "\n",
    "    # data augmentation\n",
    "    for aug in augment_image(img):\n",
    "        X_train.append(image_to_vector(aug))\n",
    "        y_train.append(row[\"label\"])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(\"Train :\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = [], []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    img = load_image_gray(row[\"path\"])\n",
    "    X_test.append(image_to_vector(img))\n",
    "    y_test.append(row[\"label\"])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Test :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA (95 % de variance conserv√©e)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train = pca.fit_transform(X_train_scaled)\n",
    "X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"Pixels initiaux :\", X_train_scaled.shape[1])\n",
    "print(\"Dimensions apr√®s PCA :\", X_train.shape[1])\n",
    "print(\"Variance expliqu√©e :\", pca.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [2000, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs param√®tres :\", grid.best_params_)\n",
    "print(\"Meilleur score CV (F1-macro) :\", grid.best_score_)\n",
    "best_lr = grid.best_estimator_\n",
    "y_pred = best_lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Logistic Regression (Best Model) =====\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred, labels=best_lr.classes_)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=best_lr.classes_,\n",
    "    yticklabels=best_lr.classes_\n",
    ")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de confusion - Logistic Regression (Best)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "PCA + extraction de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    img = load_image_gray(row[\"path\"])\n",
    "\n",
    "    # image originale\n",
    "    X_train.append(extract_hog(img))\n",
    "    y_train.append(row[\"label\"])\n",
    "\n",
    "    # data augmentation\n",
    "    for aug in augment_image(img):\n",
    "        X_train.append(extract_hog(aug))\n",
    "        y_train.append(row[\"label\"])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(\"Train :\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = [], []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    img = load_image_gray(row[\"path\"])\n",
    "    X_test.append(extract_hog(img))\n",
    "    y_test.append(row[\"label\"])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Test :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA (95 % de variance conserv√©e)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train = pca.fit_transform(X_train_scaled)\n",
    "X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"Dimensions HOG initiales :\", X_train_scaled.shape[1])\n",
    "print(\"Dimensions apr√®s PCA :\", X_train.shape[1])\n",
    "print(\"Variance expliqu√©e :\", pca.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"tol\": [1e-4, 1e-3, 1e-2],\n",
    "        \"max_iter\": [2000, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs param√®tres :\", grid.best_params_)\n",
    "print(\"Meilleur score CV (F1-macro) :\", grid.best_score_)\n",
    "best_lr = grid.best_estimator_\n",
    "y_pred = best_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Logistic Regression (Best Model) =====\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred, labels=best_lr.classes_)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=best_lr.classes_,\n",
    "    yticklabels=best_lr.classes_\n",
    ")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de confusion - Logistic Regression (Best)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdatatp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
